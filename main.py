import logging
import asyncio
from typing import Dict, List

import openai
from aiogram import Bot, Dispatcher, types, F, BaseMiddleware
from aiogram.filters import Command
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
# from aiogram.utils.markdown import code

from models import LLM_MODELS
# –í–∫–ª—é—á–∏—Ç–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(level=logging.INFO)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞
from config import (
    API_TELEGRAM_TOKEN,
    API_VSEGPT_TOKEN,
    ALLOWED_USERS,
    VSEGPT_API_BASE
)

bot = Bot(
    token=API_TELEGRAM_TOKEN,
    default=DefaultBotProperties(parse_mode=ParseMode.HTML
                                 )
)
dp = Dispatcher()

openai.api_base = VSEGPT_API_BASE
openai.api_key = API_VSEGPT_TOKEN
bot_users = [int(id_str) for id_str in ALLOWED_USERS.split(':')]
llm_model = LLM_MODELS[0]

class CommonMiddleware(BaseMiddleware):
    async def __call__(self, handler, event: types.Update, data: dict):
        message = event.message
        if message:
            await bot.send_chat_action(chat_id=message.from_user.id, action='typing')

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —á–∞—Ç–∞
            if message.chat.id not in bot_users:
                await message.reply(f"–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –≤–∞—Å –Ω–µ –∑–Ω–∞—é. –ù–µ –ø–∏—à–∏—Ç–µ –º–Ω–µ –±–æ–ª—å—à–µ.\n–ï—Å–ª–∏ —è –¥–æ–ª–∂–µ–Ω –≤–∞—Å –∑–Ω–∞—Ç—å, –ø–µ—Ä–µ–¥–∞–π—Ç–µ –º–æ–µ–º—É –≤–ª–∞–¥–µ–ª—å—Ü—É –≤–∞—à Telegram ID <code>{message.chat.id}</code>")
                return
        
        try:
            result = await handler(event, data)
            await bot.set_message_reaction(chat_id=message.from_user.id, message_id=message.message_id, reaction=[{'type':'emoji', 'emoji':'üëå'}])
            return result
        except Exception as e:
            print(f'Exception: {e}')
            await bot.set_message_reaction(chat_id=message.from_user.id, message_id=message.message_id, reaction=[{'type':'emoji', 'emoji':'ü§∑‚Äç‚ôÇ'}])
            await message.answer(f'ü§∑‚Äç‚ôÇÔ∏è –û—à–∏–±–∫–∞: {e}')
            return

dp.update.middleware(CommonMiddleware())  # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è middleware

# –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤
conversation_history = {}

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–µ–∑–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
def trim_history(history, max_length=4096):
    current_length = sum(len(message["content"]) for message in history)
    while history and current_length > max_length:
        removed_message = history.pop(0)
        current_length -= len(removed_message["content"])
    return history

async def change_model(message: types.Message, model = None) -> None:
    global llm_model
    if not model:
        # –ï—Å–ª–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –Ω–µ—Ç, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        models_list = "\n".join([f"<code>/model {model}</code>" for model in LLM_MODELS])
        await message.answer(
            f"–°–µ–π—á–∞—Å –≤—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å {llm_model}" + 
            "\n\n–ù–∞–∂–º–∏ –Ω–∞ –∫–æ–º–∞–Ω–¥—É –≤—ã–±–æ—Ä–∞ –æ–¥–Ω–æ–π –∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —á—Ç–æ–±—ã —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –µ—ë, –∞ –∑–∞—Ç–µ–º –æ—Ç–ø—Ä–∞–≤—å —ç—Ç—É –∫–æ–º–∞–Ω–¥—É –º–Ω–µ:\n\n" + models_list +
            "\n\n–ú–æ–∂–Ω–æ –Ω–∞–ø–∏—Å–∞—Ç—å –≤ –∫–æ–º–∞–Ω–¥–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤—Ä—É—á–Ω—É—é, –Ω–æ –µ—Å–ª–∏ —Ç–∞–∫–æ–π –º–æ–¥–µ–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Ç–æ –≤–º–µ—Å—Ç–æ –æ—Ç–≤–µ—Ç–∞ —è –≤–µ—Ä–Ω—É –æ—à–∏–±–∫—É. –í —Ç–∞–∫–æ–º —Å–ª—É—á–∞–µ –≤—ã–±–µ—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –º–æ–¥–µ–ª—å."
        )
        return
    
    llm_model = model
    await message.answer(f"–ú–æ–¥–µ–ª—å –∏–∑–º–µ–Ω–µ–Ω–∞ –Ω–∞ <b>{llm_model}</b>.")

@dp.message(Command('start'))
async def process_start_command(message: types.Message):
    commands_info = """
–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ –ò–ò-–±–æ—Ç!

–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:
/start - –ø–æ–∫–∞–∑–∞—Ç—å —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ
/clear - –æ—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞
/model - –ø—Ä–æ—Å–º–æ—Ç—Ä –∏ –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –ò–ò

–ü—Ä–æ—Å—Ç–æ –æ—Ç–ø—Ä–∞–≤–ª—è–π—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏—è, –∏ —è –±—É–¥—É –Ω–∞ –Ω–∏—Ö –æ—Ç–≤–µ—á–∞—Ç—å!
"""
    await message.answer(commands_info)


@dp.message(Command('clear'))
async def process_clear_command(message: types.Message):
    user_id = message.from_user.id
    conversation_history[user_id] = []
    await message.reply("–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ –æ—á–∏—â–µ–Ω–∞.")


@dp.message(Command('model'))
async def process_model_command(message: types.Message):
    """Handle model selection command"""
    
    # –ü–æ–ª—É—á–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç –∫–æ–º–∞–Ω–¥—ã
    args = message.text.split()[1:]
    model = args[0] if len(args) > 0 else None
    await change_model(message, model)    

@dp.message(Command('sonnet'))
async def process_model_sonnet_command(message: types.Message):
    await change_model(message, "anthropic/claude-3.5-sonnet")

@dp.message(Command('haiku'))
async def process_model_haiku_command(message: types.Message):
    await change_model(message, "anthropic/claude-3-5-haiku")

@dp.message(Command('gpt'))
async def process_model_haiku_command(message: types.Message):
    await change_model(message, "openai/gpt-4o-latest")

@dp.message(F.text)
async def process_message(message: types.Message):
    user_id = message.from_user.id
    if user_id not in bot_users:
        await message.answer("–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –≤–∞—Å –Ω–µ –∑–Ω–∞—é. –ù–µ –ø–∏—à–∏—Ç–µ –º–Ω–µ –±–æ–ª—å—à–µ")
        return
    
    user_input = message.text

    if user_id not in conversation_history:
        conversation_history[user_id] = []

    conversation_history[user_id].append({"role": "user", "content": user_input})
    conversation_history[user_id] = trim_history(conversation_history[user_id])

    chat_history = conversation_history[user_id]

    try:
        response = await openai.ChatCompletion.acreate(
            model=llm_model,
            messages=chat_history

        )
        chat_gpt_response = response["choices"][0]["message"]["content"]
    except Exception as e:
        print(e)
        chat_gpt_response = f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}"

    conversation_history[user_id].append({"role": "assistant", "content": chat_gpt_response})
    print(conversation_history)
    length = sum(len(message["content"]) for message in conversation_history[user_id])
    print(length)
    await message.answer(chat_gpt_response)

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫–æ–º–∞–Ω–¥ –±–æ—Ç–∞
async def set_commands():
    commands = [
        types.BotCommand(command="start", description="–ù–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É —Å –±–æ—Ç–æ–º"),
        types.BotCommand(command="clear", description="–°–º–µ–Ω–∏—Ç—å —Ç–µ–º—É –∏ –Ω–∞—á–∞—Ç—å –Ω–æ–≤—ã–π –¥–∏–∞–ª–æ–≥"),
        types.BotCommand(command="sonnet", description="–í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å anthropic/claude-3.5-sonnet"),
        types.BotCommand(command="haiku", description="–í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å anthropic/claude-3-5-haiku"),
        types.BotCommand(command="gpt", description="–í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å openai/gpt-4o-latest"),
        types.BotCommand(command="model", description="–í—ã–±—Ä–∞—Ç—å –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å –ò–ò"),
    ]
    await bot.set_my_commands(commands)

# –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
async def main():
    await set_commands()  # –î–æ–±–∞–≤–ª—è–µ–º —ç—Ç—É —Å—Ç—Ä–æ–∫—É
    await dp.start_polling(bot)

if __name__ == '__main__':
    asyncio.run(main())
